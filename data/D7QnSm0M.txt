# -*- coding: utf-8 -*-

"""
Buscador Web

Realizar solicitudes HTTP para recuperar información de Internet, se deberá
importar la biblioteca `urllib` y de ella el módulo `request`.

El módulo `urllib.request` realiza una solicitud HTTP para recuperar la 
información de una página Web.

Por ejemplo, para recuperar el HTML de una página web, puede usar la función
`urlopen()`, en este caso usamos 'https://www.python.org/' como `objetivo`.

Esto enviará una solicitud HTTP GET a la URL especificada y recuperará el HTML
de la página web. Luego, el HTML se almacena en la variable `html`.

     Analizar el HTML para extraer la información que desea:

Ahora que tiene el HTML de la página web, deberá analizarlo para extraer la
información que desea. Puede usar una biblioteca como `BeautifulSoup` para
analizar el HTML y extraer la información.

Para instalar `BeautifulSoup`, deberá ejecutar el siguiente comando:

    $ pip install beautifulsoup4

Luego puede usar el método `find()` para buscar elementos específicos en el
HTML y extraer todos los enlaces del HTML.

[soup.find_all('a')] encuentra todos los elementos `a`
(es decir, los enlaces) en el HTML y los almacenará en la lista de `links`.

     Almacene o muestre la información:

Una vez que haya extraído la información que desea, puede almacenarla en
un archivo o base de datos, o mostrársela en pantalla.

Usamos la función `print()` para mostrar los enlaces que extrajo:

En este caso vamos a redirigir la salida de la función `print()` a
un archivo de texto previamente creado: `resultado.txt`

Creado con `VSCodium` por @alpfa, dom 08 ene 2023

"""

import urllib.request

response = urllib.request.urlopen('https://www.python.org/')
html = response.read()

from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')

links = soup.find_all('a')

with open("/home/alpfa/enviroments/educatio/resultado.txt", 'w') as resultado:
    for link in links:
        print(link.get('href'), file=resultado)
