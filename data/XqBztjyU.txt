class setka(nn.Module):
    
    def init_weights(m):
        if isinstance(m, nn.Linear):
            torch.nn.init.kaiming_normal_(m.weight)
            m.bias.data.fill_(0.01)   
    
    def __init__(self, in_features, power, hiddens, act):
        super(setka, self).__init__()
        
        self.in_features = in_features
        self.power = power
        self.out_features = math.floor(self.in_features/self.power)
        self.hiddens = hiddens
        self.act = act
        
        self.net_layers = []
        self.net_layers.append(nn.Linear(self.in_features, self.out_features))
        self.net_layers.append(self.act)
        self.for_bn = self.out_features
        
        for i in range(self.hiddens):
            
            self.in_features = self.out_features
            self.out_features = math.floor(self.in_features/self.power)
            self.net_layers.append(nn.Linear(self.in_features, self.out_features))
            self.net_layers.append(self.act)
                
        self.net_layers.append(nn.Linear(self.out_features, 2))
        self.net_layers.append(nn.Sigmoid())
        self.net_layers.insert(1, nn.BatchNorm1d(self.for_bn))
        self.net = nn.Sequential(*self.net_layers)
        self.init_weights()
        
    def forward(self, x):
        out = self.net.forward(x)
        return out

class EarlyStopping():
    
    def __init__(self, patience=5, min_delta=0):

        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss == None:
            self.best_loss = val_loss
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        elif self.best_loss - val_loss < self.min_delta:
            self.counter += 1
            print(f"INFO: Early stopping counter {self.counter} of {self.patience}")
            if self.counter >= self.patience:
                print('INFO: Early stopping')
                self.early_stop = True

def training(purpose: 'str', optimizer : object, model: object, batches: int, 
             X_train: t.Tensor, X_val: t.Tensor, y_train: t.Tensor, y_val: t.Tensor) -> dict:
    
    use_cuda = t.cuda.is_available()
    device = t.device("cuda" if use_cuda else "cpu")
    
    model.to(device)

    loss = nn.BCEWithLogitsLoss()

    num_epochs = 10000
    
    train_data = DataLoader(TensorDataset(X_train, y_train), batch_size=batches, shuffle=True)
    early_stopping = EarlyStopping(patience=4, min_delta=20)
        
    train_losses = []
    test_losses = []
    
    for epoch in range(num_epochs):
        
        total_train_loss = 0
        total_test_loss = 0
        
        for X_train, y_train in train_data:
            
            optimizer.zero_grad()
            model.train()
            
            X_train = X_train.to(device)
            y_train = y_train.to(device)
        
            preds = model(X_train).flatten()
            
            print(preds.shape)
            print(y_train.shape)
        
            loss_train = loss(preds, y_train.flatten())
            total_train_loss += loss_train.item()
            
            loss_train.backward()
            optimizer.step()
                
        with t.no_grad():
            
            model.eval()
            X_val = X_val.to(device)
            y_val = y_val.to(device)
            
            test_preds = model(X_val).flatten()
            loss_test = loss(test_preds, y_val.flatten())
            total_test_loss += loss_test.item()
            
        train_losses.append(total_train_loss/len(train_data))
        test_losses.append(total_test_loss)
        
        if epoch % 100 == 0:
            print(f'Epoch: {epoch}, CEL val: {total_test_loss}')
            early_stopping(total_test_loss)
            if early_stopping.early_stop:
                print('Early Stopping!')
                break
                
        if epoch == 1:
            ref = total_test_loss
        elif epoch == 1000:
            check = total_test_loss
            if ref/check < 1.5:
                print(f'Too bad: decay is {round(ref/check,3)}')
                break
                
    if purpose == 'tuning':

        plt.plot(train_losses, label='train')
        plt.plot(test_losses, label='val')
        plt.title("Baseline")
        plt.legend()
        plt.show()
    
    if purpose == 'tuning':
    
        return {'train_loss' : train_losses,
               'val_loss' : test_losses}
    
    elif purpose == 'export': 
        
        return model

def objective(trial):
    
    batch_size = 10000
    
    model = setka(X_train.shape[1], 
                  trial.suggest_int('power',1,3), 
                  trial.suggest_int('hiddens',1,4), 
                  trial.suggest_categorical('act', [nn.LeakyReLU(), nn.SiLU(), nn.Softmax()]))
    
    optimizer = t.optim.Adam(model.parameters(), lr = trial.suggest_float('lr',0.05,0.1), weight_decay=1e-4)
    
    CLE = training('tuning', optimizer, model, batch_size, *data_post)['val_loss'][-1]
    
    return CLE

study = opt.create_study(direction="minimize")
study.optimize(objective, n_trials=30)