def write_chunks(self,file_id:str,file_date:str=None,ext='csv'):
    """
    Reads output from oracle table, and writes output to xlsx file for each set of chunks
    """
    sql_file_id = "'"+file_id+"'"
    if file_date:
        # getting the date to be applied to the file and oracle query
        file_date, sql_exec_time = get_file_dates(file_date)
    else:
        sql_exec_time = conf['sql_exec_time']
        file_date = self.file_date
    # querying results using read_sql. output will be chunked by self.chunksize
    try:
        engine = create_engine(self.connstr)
        conn = engine.connect().execution_options(stream_results=True)
        chunks = read_sql(
            conf['qry_ora_main'].format(
                sql_exec_time = sql_exec_time
                ,file_id = sql_file_id)
            ,conn
            ,chunksize=self.chunksize
            )   
    # throw an error if something with the connection went wrong
    except ConnectionError as e:
        self.logger.error(e)
        raise e

    date = file_date.strftime("%Y-%m-%d")
    try:
        # iterating through generator chunks
        for idx,records in enumerate(chunks,start=1):
            # If there wasn't any records returned, break without creating an output file.
            if len(records) == 0:
                self.logger.info(f"No records were returned for file id {file_id}")
                break
            # sets the headers of the df to uppercase
            set_df_header_casing(records)
            # if there are more records than the chunksize, create an autoincremented fileid
            if len(records) < self.chunksize and idx==1:
                filename = f'{file_id} {date}.{ext}'
            else:
                filename = f'{file_id}_{idx} {date}.{ext}'
            # writing the results to the windows share
            dest = os.path.join(conf['smb']['share'],conf['smb']['path'],filename)
            # creating an in memory bytes stream to write the xlsx output to
            data = io.BytesIO()
            if ext=='csv':
                records.to_csv(data,chunksize=self.chunksize,index=False,lineterminator='\r\n')
            elif ext=='xlsx':
                try:
                    # writing the results to the binary stream
                    with ExcelWriter(data,engine='xlsxwriter') as writer:
                        self.logger.info(f"Writing binary stream results for {file_id} ")
                        ### we never make it past this point
                        records.to_excel(writer,index=False,sheet_name='Detailed_Record_Data')
                        data.seek(0)
                except Exception as e:
                    print(e)
            else:
                raise ValueError(f"{ext} is not a valid extension.")
            try:
                write_to_share_drive(data,filename)
            except Exception as e:
                self.logger.error(f"Something went wrong writing to the SMB share. Check {conf['smb']['server']}/{conf['smb']['share']}/{dest} and ensure the file is not open.")
                self.logger.critical(e)
                raise e
    except Exception as e:
        print(e)
    conn.close()
    engine.dispose()

def run_job(self,workers:int=20,**kwargs):
    """
    collects a list of file_ids. file_ids are be split up into their own thread, 
    mapping to write_chunks(). file_ids is plugged into a oracle query such as:
     `SELECT somCols from someTbl where fileid = 'file_id'`
    """
    self.logger.info(f"Threads: {workers}")
    if not hasattr(self, 'file_ids'):
        self.get_file_ids()
    # creating args to be mapped to each file_id
    args = [{'file_id':file,**kwargs} for file in self.file_ids]
    # creating threads for each file_id
    with concurrent.futures.ThreadPoolExecutor(workers) as exe:
        results = exe.map(lambda x: self.write_chunks(**x),args)
    # returning results
    for result in results:
        if result:
            self.logger.info(result)
    self.logger.info('Job completed!')