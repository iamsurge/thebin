from keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
data = fashion_mnist.load_data()
X_train, y_train = data[0][0], data[0][1]
X_test, y_test = data[1][0], data[1][1]
X_train = np.expand_dims(X_train, axis = -1)
X_test = np.expand_dims(X_test, axis = -1)
y_train = pd.get_dummies(pd.Categorical(y_train)).values
y_test = pd.get_dummies(pd.Categorical(y_test)).values
from matplotlib import pyplot as plt
from matplotlib import rcParams
rcParams['font.size'] = 48
def show_pictures(arrs):
 arr_cnt = arrs.shape[0]
 fig, axes = plt.subplots(1, arr_cnt, figsize=(5*arr_cnt, arr_cnt))
 for axis, pic in zip(axes, arrs):
     axis.imshow(pic.squeeze(), cmap = 'gray')
     axis.axis('off')
 fig.tight_layout()
demo_images = X_train[:10,...,0]
# show_pictures (demo_images).suptitle("ZdjÄ™cia pierwotne")
# odbicia_poziome = demo_images[...,::-1]
# show_pictures(odbicia_poziome).suptitle("Odbicia poziome")
# odbicia_pionowe = demo_images[...,::-1,:]
# show_pictures(odbicia_pionowe).suptitle("Odbicia pionowe")
from PIL import Image
rotated_images = demo_images.copy()
img_size = demo_images.shape[1:]
angles = np.random.randint(-30,30, len(rotated_images))
for i, img in enumerate(rotated_images):
    angle = np.random.randint(-30,30)
    img = Image.fromarray(img).rotate(angle,
    expand = 1).resize(img_size)
    rotated_images[i] = np.array(img)
# show_pictures(rotated_images)

from PIL import Image
rotated_images = demo_images.copy()
img_size = demo_images.shape[1:]
for i, img in enumerate(rotated_images):
 angle = np.random.randint(-30,30)
 left, upper = np.random.randint(0, 5, 2)
 right, lower = np.random.randint(23, 28, 2)
 img = Image.fromarray(img).crop((left, upper, right, lower)).resize(img_size) 
 rotated_images[i] = np.array(img)
show_pictures(rotated_images)

from keras.models import Model
from keras.layers import Input, Dense, Dropout, Reshape, BatchNormalization, Lambda
from keras.optimizers import Adam
act_func = 'selu'
hidden_dims = 64

encoder_layers = [
 Reshape((28*28,)),
 BatchNormalization(),
 Dense(512,activation=act_func),
 Dense(128,activation=act_func),
 Dense(64, activation=act_func),
 Dense(hidden_dims, activation=act_func)]
tensor = encoder_input = Input(shape = (28,28))
for layer in encoder_layers:
 tensor = layer(tensor)
encoder_output = tensor
encoder = Model(inputs=encoder_input,
 outputs=encoder_output)

decoder_layers = [
 Dense(128,activation=act_func),
 Dense(512,activation=act_func),
 Dense(784,activation='sigmoid'),
 Reshape((28,28)),
 Lambda(lambda x: x*255)]
decoder_input = tensor = Input(encoder_output.shape)
for layer in decoder_layers:
 tensor = layer(tensor)
decoder_output = tensor
decoder = Model(inputs = decoder_input, outputs = decoder_output)

learning_rate = 0.0001;
aec_output = decoder(encoder(encoder_input))
gen_autoencoder = Model(inputs = encoder_input, outputs = aec_output)
gen_autoencoder.compile(optimizer = Adam(learning_rate), loss = 'MeanSquaredError')
gen_autoencoder.fit(x=X_train,y=X_train,
 validation_data=(X_test, X_test),
 batch_size=256, epochs=10)

from keras import backend as K
def adding_noise(tensor):
 noise = K.random_normal(shape=(K.shape(tensor)),
 mean=0, stddev=1.5)
 return tensor + noise
noised_encoder_output = Lambda(adding_noise)(encoder_output)
augmenter_output = decoder(noised_encoder_output)
augmenter = Model(inputs = encoder_input, outputs = augmenter_output)

def filter_data(data, iteration_num):
 augmented_data = data.copy()
 for i in range(iteration_num):
    augmented_data = gen_autoencoder.predict(augmented_data)
 return augmented_data
start = 50
end = start + 10
for i in range(10):
 test_for_augm = X_train[i*10:i*10+10,...]
 augmented_data = test_for_augm.copy()
 show_pictures(test_for_augm)
 augmented_data = augmenter.predict(augmented_data)
 show_pictures(augmented_data)
 augmented_data = filter_data(augmented_data, 5)
 show_pictures(augmented_data)

